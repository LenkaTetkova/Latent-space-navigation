{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1bf0c2d4",
      "metadata": {
        "id": "1bf0c2d4"
      },
      "source": [
        "# Sparse Autoencoders for model steering\n",
        "Authored by Mikkel Godsk JÃ¸rgensen (mgojo@dtu.dk)\n",
        "\n",
        "In this notebook, we will explore using sparse autoencoders (abbr.: *SAEs*) for model steering in a similar fashion to [Templeton et al., 2024](https://transformer-circuits.pub/2024/scaling-monosemanticity/). We will be using the Gemma-2-2b model by Google [(Riviere et al., 2024)](https://arxiv.org/pdf/2408.00118), and the Gemma-Scope suite by DeepMind [(Lieberum et al., 2024)](https://arxiv.org/pdf/2408.05147).\n",
        "\n",
        "A sparse autoencoder is a shallow autoencoder with a wide intermediate layer trained to activate sparsely. In Gemma-Scope architecture it is as follows:\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\textbf{Encoder:}\\quad\\quad\\boldsymbol{f}(\\boldsymbol{x})&=\\sigma(\\boldsymbol{W}_{\\rm enc}\\boldsymbol{x}+\\boldsymbol{b}_{\\rm enc})\\\\\n",
        "    \\textbf{Decoder:}\\quad\\quad\\boldsymbol{g}(\\boldsymbol{f})&=\\boldsymbol{W}_{\\rm dec}\\boldsymbol{f}+\\boldsymbol{b}_{\\rm dec}.\n",
        "\\end{split}\n",
        "$$\n",
        "Here the input $\\boldsymbol{x}\\in\\mathbb{R}^n$ is an internal representation from the model subject to investigation/steering (i.e. Gemma 2), $\\boldsymbol{f}(\\boldsymbol{x})\\in\\mathbb{R}^M$ with $M\\gg n$ is a vector of so-called *feature activations*, and $\\hat{\\boldsymbol{x}}=\\boldsymbol{g}(\\boldsymbol{f}(\\boldsymbol{x}))\\in\\mathbb{R}^n$ is the reconstruction of the internal representation.\n",
        "$\\sigma:\\mathbb{R}^M\\rightarrow\\mathbb{R}^M$ is the $\\textrm{JumpReLU}_{\\boldsymbol{\\theta}}$ activation function parametrized by $\\boldsymbol{\\theta}$. \n",
        "\n",
        "The model is trained in a fashion to minimize $\\mathcal{L}=||\\boldsymbol{x}-\\hat{\\boldsymbol{x}}||_2^2+\\lambda ||\\boldsymbol{f}(\\boldsymbol{x})||_0.$ Here the second term incentivizes sparsity on the feature activations.\n",
        "For those wanting a more detailed technical explanation, refer to e.g. [Rajamanoharan et al., 2024](https://arxiv.org/pdf/2407.14435).\n",
        "\n",
        "\n",
        "In Gemma-2, the latent representation has 2304 dimensions while the SAE we'll be using has 65536 dimensions - so we're expanding a 2304 dimensional real vectorspace into a 65536 dimensional one where each dimension is used sparsely.\n",
        "\n",
        "... But why would we do this?<br>\n",
        "To answer this question, we could start by considering the vast knowledge that large language models seem to possess, in spite of having a relatively moderate size of their hidden dimension. Although debated, it has even been hypothesized that much of the knowledge of many deep learning models is represented linearly in what is called the *Linear representation hypothesis* (abbr.: *LRH*). In an effort to explain the perceived vast knowledge encoded in LLMs despite their (relatively) low-dimensional hidden states, it has been proposed that LLMs utilize something called *superposition* where different concept directions need not be orthogonal (see e.g. [Elhage et al., 2022](https://transformer-circuits.pub/2022/toy_model/index.html) if you are curious), which, in turn, implies that single neurons don't respond to single concepts but are instead *polysemantic*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540a32a0",
      "metadata": {
        "id": "540a32a0"
      },
      "source": [
        "## Getting started\n",
        "**Compute requirements:**\n",
        "It is recommended to upload this notebook to Google Colab and to select a GPU instance (the free-tier T4 is sufficient). You can do that by clicking [here](https://colab.research.google.com/github/LenkaTetkova/Latent-space-navigation/blob/main/d3a_sae_notebook.ipynb).\n",
        "\n",
        "**Library installation:**\n",
        "Please run the cell below to pip-install the necessary libraries in Google Colab, and restart the runtime when it asks you to do so.\n",
        "While you're waiting for the installation to finish, please carry out the next step.\n",
        "\n",
        "**Access:**\n",
        "To get access to download Gemma 2 2b, you must sign up at HuggingFace, apply for the model [here](https://huggingface.co/google/gemma-2-2b-it), and create an access token by following these [instructions](https://huggingface.co/docs/hub/security-tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ROMMTbR8Mb3z",
      "metadata": {
        "id": "ROMMTbR8Mb3z"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !pip install transformers bitsandbytes sae_lens==6.6.0 datasets --upgrade\n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q3_omAErYYjA",
      "metadata": {
        "id": "Q3_omAErYYjA"
      },
      "source": [
        "**Important:** Please restart the session before running the next cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f474d46c",
      "metadata": {
        "id": "f474d46c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from sae_lens.saes.sae import SAE\n",
        "\n",
        "access_token = \"hf_...\"     # Ideally, this should be stored in a secret\n",
        "\n",
        "model_name = \"google/gemma-2-2b-it\"\n",
        "device = \"cuda:0\"   # \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token,)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device,\n",
        "    token=access_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fd7b3a",
      "metadata": {
        "id": "78fd7b3a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import io\n",
        "import json\n",
        "response = requests.get(\"https://github.com/LenkaTetkova/Latent-space-navigation/raw/refs/heads/main/data/sae_feature_labels.json\")\n",
        "feature_label_dict = json.load(io.BytesIO(response.content))\n",
        "\n",
        "sae = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=f\"layer_21/width_65k/average_l0_20\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34307397",
      "metadata": {
        "id": "34307397"
      },
      "source": [
        "## Model steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c46c30",
      "metadata": {
        "id": "54c46c30"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, List\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def clamp_intervention(\n",
        "        latents:torch.Tensor,   # [batch_size, seq_len, vocab_size]\n",
        "        feature_ixs: List[int],\n",
        "        clamp_value: float,\n",
        "    ) -> torch.Tensor:          # -> [batch_size, seq_len, vocab_size]\n",
        "        mask = torch.zeros((latents.shape[-1],), dtype=latents.dtype, device=latents.device)[None,None,:]\n",
        "        mask[...,feature_ixs] = 1.\n",
        "        return (latents * (1.-mask)) + clamp_value*mask\n",
        "\n",
        "\n",
        "class SAEIntervention:       # To be used in the intervention forward as the `intervention_fun`.\n",
        "    def __init__(self, sae:SAE, intervention:Callable[[torch.Tensor], torch.Tensor] = lambda x:x,):\n",
        "        self.sae = sae\n",
        "        self.intervention = intervention\n",
        "\n",
        "    def __call__(\n",
        "            self,\n",
        "            acts:torch.Tensor   # [batch_size, seq_len, hidden_dim]\n",
        "        ) -> torch.Tensor:      # -> [batch_size, seq_len, hidden_dim]\n",
        "        error = acts - self.sae.forward(acts).to_dense()\n",
        "        latents = self.sae.encode(acts).to_dense()\n",
        "        new_latents = self.intervention(latents)\n",
        "        acts_intervention = self.sae.decode(new_latents)\n",
        "        acts_hat = error + acts_intervention\n",
        "        return acts_hat.to(acts.dtype)\n",
        "\n",
        "\n",
        "class CAVIntervention:\n",
        "    def __init__(self, cav:torch.Tensor, scale:float):\n",
        "        self.cav = cav.flatten()\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(\n",
        "            self,\n",
        "            acts:torch.Tensor   # [batch_size, seq_len, hidden_dim]\n",
        "        ) -> torch.Tensor:      # -> [batch_size, seq_len, hidden_dim]\n",
        "        return (acts + self.scale * self.cav[None,None,:]).to(acts.dtype)\n",
        "\n",
        "\n",
        "class InterventionForwardHook:\n",
        "    def __init__(self, intervention_fun: Callable[[torch.Tensor], torch.Tensor]):\n",
        "        self.intervention_fun = intervention_fun\n",
        "\n",
        "    def __call__(self, module, args, outputs: torch.Tensor):\n",
        "        return (self.intervention_fun(outputs[0]),)\n",
        "\n",
        "\n",
        "def clear_all_hooks(model):\n",
        "    for m in model.modules():\n",
        "        m._forward_hooks.clear()\n",
        "\n",
        "\n",
        "def generate(input_prompt, intervention_layer, intervention):\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": input_prompt}\n",
        "        ],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Add intervention to model\n",
        "    handle = model.model.layers[intervention_layer].register_forward_hook(\n",
        "        InterventionForwardHook(\n",
        "            intervention,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Inference, clean up, decode\n",
        "    outputs = model.generate(input_ids, max_new_tokens=64, do_sample=True)\n",
        "    handle.remove()     # Clean up forward hook\n",
        "    outputs = outputs[0][input_ids.shape[1]:]   # Remove prompt\n",
        "    return tokenizer.decode(outputs)     # Ensure the LLM is able to speak"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92bb7d91",
      "metadata": {
        "id": "92bb7d91"
      },
      "source": [
        "## Experiment time!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34762084",
      "metadata": {
        "id": "34762084"
      },
      "source": [
        "### Steering with Sparse Autoencoders\n",
        "To steer the model with sparse autoencoders, we edit the activations during inference and take the approach of [Templeton et al., 2024](https://transformer-circuits.pub/2024/scaling-monosemanticity/). Here we let $\\boldsymbol{x}\\in\\mathbb{R}^n$ denote the model activations in a particular layer.\n",
        "\n",
        "We start by computing the feature activations $\\boldsymbol{f}(\\boldsymbol{x})$ using the SAE. We then compute the reconstruction error term\n",
        "$$\n",
        "\\boldsymbol{e}=\\boldsymbol{x}-\\hat{\\boldsymbol{x}}.\n",
        "$$\n",
        "\n",
        "To steer using feature #i and a strength $\\alpha$, compute\n",
        "$$\n",
        "\\boldsymbol{f}_{\\rm int}=\\boldsymbol{f}(\\boldsymbol{x})\\odot (1-\\boldsymbol{m})+\\alpha\\boldsymbol{m}\n",
        "$$\n",
        "where $\\boldsymbol{m}$ has the elements $$m_j=\\begin{cases}1&\\textrm{ if }j=i\\\\0&\\textrm{ otherwise}\\end{cases}.$$ Here $\\odot$ denotes the elementwise product.\n",
        "\n",
        "We now define the edited \"reconstruction\" as $\\hat{\\boldsymbol{x}}_{\\rm int}=\\boldsymbol{g}(\\boldsymbol{f}_{\\rm int})+\\boldsymbol{e}$ which we now pass through the rest of the model. In essence, what we have just done is to force a specific activation pattern into the models inference, which will affect the rest of the computations as we go further downstream!\n",
        "\n",
        "\n",
        "Since SAEs are trained unsupervised, their features don't come with a ground truth label attached - we must find those ourselves. There have some different proposed pipelines using LLMs to guess what a feature responded to via the maximally activating samples. You can explore this approach [here](https://www.neuronpedia.org/gemma-scope) but note that their choice of SAE for layer 21 has a different sparsity parameter from ours rendering their labels incompatible with this notebook.\n",
        "\n",
        "\n",
        "The labels provided in this notebook stem from our own auto-labelling pipeline. Keep in mind that mismatches between features and labels may occur.\n",
        "\n",
        "\n",
        "Try for yourself to see if you can steer the model in a way you find favorable by adjusting the feature and strength. You will likely find that setting the strength to a high value will break the model, while a lower value may make it talk about your chosen topic without you soliciting it in the prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2041f0e",
      "metadata": {
        "id": "c2041f0e"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "\n",
        "intervention_layer = int(sae.cfg.metadata['hook_name'].split('.')[1])\n",
        "widget_feature_select = widgets.Select(\n",
        "    options=list(feature_label_dict.keys()),\n",
        "    value='baseball',\n",
        "    description='Feature:',\n",
        "    disabled=False\n",
        ")\n",
        "widget_clamp_select = widgets.FloatSlider(\n",
        "    value=525.0,\n",
        "    min=0,\n",
        "    max=1000.0,\n",
        "    step=25.0,\n",
        "    description='Clamp value:',\n",
        "    disabled=False,\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.1f',\n",
        ")\n",
        "widget_text_input = widgets.Textarea(\n",
        "    value='Write a haiku.',\n",
        "    placeholder='Type in a prompt',\n",
        "    description='Prompt:',\n",
        "    disabled=False\n",
        ")\n",
        "w = interactive(\n",
        "    lambda **kwargs: print(\n",
        "        generate(\n",
        "            input_prompt=kwargs[\"input_prompt\"],\n",
        "            intervention_layer=intervention_layer,\n",
        "            intervention=SAEIntervention(\n",
        "                sae=sae,\n",
        "                intervention=partial(\n",
        "                    clamp_intervention,\n",
        "                    feature_ixs=[feature_label_dict[kwargs[\"feature_topic\"]]],\n",
        "                    clamp_value=kwargs[\"clamp_value\"],\n",
        "                )\n",
        "            )\n",
        "        ).strip()\n",
        "    ),\n",
        "    {'manual': True, 'manual_name': \"Generate\"},\n",
        "    feature_topic=widget_feature_select,\n",
        "    clamp_value=widget_clamp_select,\n",
        "    input_prompt=widget_text_input\n",
        ")\n",
        "ui = widgets.HBox(w.children[:-1])\n",
        "out = w.children[-1]\n",
        "display(widgets.VBox([ui, out]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2baf0492",
      "metadata": {
        "id": "2baf0492"
      },
      "source": [
        "### Steering via CAVs\n",
        "Another approach to steering is to use Concept Activation Vectors (abbr.: *CAVs*, see e.g. [Kim et al., 2018](https://arxiv.org/pdf/1711.11279)). This approach exhibits some similarity to the approach with the sparse autoencoders, but here the vectors come with a ground truth attached.\n",
        "To obtain CAVs, I have trained a suite on ordinary linear SVMs on the activations of the model. To aggregate the activations over a piece of text, I average the LLM representation over all tokens, which seems to work reasonably well.\n",
        "\n",
        "To intervene, I do the following:\n",
        "$$\n",
        "\\boldsymbol{x}_{\\rm int}=\\boldsymbol{x}+\\alpha\\boldsymbol{v},\n",
        "$$\n",
        "where $\\boldsymbol{v}$ is a CAV.\n",
        "\n",
        "Further down, you have the option to train your own CAVs on the 20 Newsgroup dataset if you desire to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22eeda41",
      "metadata": {
        "id": "22eeda41"
      },
      "outputs": [],
      "source": [
        "# Download CAVs from our Github repository and load them\n",
        "response = requests.get(\"https://github.com/LenkaTetkova/Latent-space-navigation/raw/refs/heads/main/data/cavs_layer_21.pt\")\n",
        "f = torch.load(io.BytesIO(response.content))\n",
        "cavs = f[\"cavs\"]            # Shape: [n_labels, hidden_dim]\n",
        "cav_labels = f[\"labels\"]    # List of length n_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df54013e",
      "metadata": {
        "id": "df54013e"
      },
      "outputs": [],
      "source": [
        "widget_topic_select_cav = widgets.Select(\n",
        "    options=cav_labels,\n",
        "    description='CAV:',\n",
        "    disabled=False\n",
        ")\n",
        "widget_select_cav_strength = widgets.FloatSlider(\n",
        "    value=525.0,\n",
        "    min=0,\n",
        "    max=1000.0,\n",
        "    step=25.0,\n",
        "    description='Strength:',\n",
        "    disabled=False,\n",
        "    continuous_update=False,\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.1f',\n",
        ")\n",
        "widget_text_input_cav = widgets.Textarea(\n",
        "    value='Write a haiku.',\n",
        "    placeholder='Type in a prompt',\n",
        "    description='Prompt:',\n",
        "    disabled=False\n",
        ")\n",
        "w_cav = interactive(\n",
        "    lambda **kwargs: print(\n",
        "        generate(\n",
        "            input_prompt=kwargs[\"input_prompt\"],\n",
        "            intervention_layer=intervention_layer,\n",
        "            intervention=CAVIntervention(\n",
        "                cavs[cav_labels.index(kwargs[\"topic\"])],\n",
        "                kwargs[\"strength_value\"],\n",
        "            )\n",
        "        ).strip()\n",
        "    ),\n",
        "    {'manual': True, 'manual_name': \"Generate\"},\n",
        "    topic=widget_topic_select_cav,\n",
        "    strength_value=widget_select_cav_strength,\n",
        "    input_prompt=widget_text_input_cav,\n",
        ")\n",
        "ui_cav = widgets.HBox(w_cav.children[:-1])\n",
        "out_cav = w_cav.children[-1]\n",
        "display(widgets.VBox([ui_cav, out_cav]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0f403c",
      "metadata": {
        "id": "7f0f403c"
      },
      "source": [
        "### Optional: Training your own CAVs\n",
        "**Note:** This is mostly for demonstration purposes, it will take a lot of time to run on a T4 GPU in Colab.\n",
        "\n",
        "This code demonstrates how you might go about training your own CAVs. The approach is similar to how I obtained the preloaded ones, but are computed on a more easily accessible dataset.\n",
        "The training will take some time and the quality may vary. To import your trained CAVs into the above widget, simply rerun the widget cell to load them in.\n",
        "\n",
        "\n",
        "Note: In the training code, I remove 64 prefix tokens (+ one BOS token) since the semantics in this dataset seem to often come a bit late in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec857767",
      "metadata": {
        "id": "ec857767"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "################################\n",
        "# Load dataset: 20 news groups #\n",
        "################################\n",
        "ds = load_dataset(\"SetFit/20_newsgroups\", trust_remote_code=False)\n",
        "dl = torch.utils.data.DataLoader(\n",
        "    ds[\"train\"],\n",
        "    batch_size=32,\n",
        "    shuffle=False,  # Deterministic ordering...\n",
        ")\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2-2b\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=device,\n",
        "    token=access_token,\n",
        ")   # The base model seems to give better results.\n",
        "\n",
        "\n",
        "#############################\n",
        "# Collect model activations #\n",
        "#############################\n",
        "@torch.no_grad()    # To avoid CUDA OOM\n",
        "def get_mean_activation(batch, max_seq_len=128, aggregate_from_token=64):\n",
        "    # Tokenize\n",
        "    tokenizer.padding_side = \"right\"    # So we can easily remove BOS and other prefix tokens\n",
        "    tokens = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
        "    tokens = {k:v[:,:max_seq_len+1,...].to(device) for k,v in tokens.items()}\n",
        "\n",
        "    # Compute model activations and remove BOS\n",
        "    activations = model_base(**tokens, output_hidden_states=True).hidden_states[intervention_layer+1][:,aggregate_from_token+1:,:]\n",
        "    mask = tokens[\"attention_mask\"][:,aggregate_from_token+1:]\n",
        "\n",
        "    # Return average activation across sequence\n",
        "    return torch.einsum('ijk,ij->ik', activations.float(), mask.float()) / mask.sum(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "activations = []\n",
        "labels = []\n",
        "for batch in tqdm(dl, desc=\"Collecting activations from language model\"):\n",
        "    activations.append(\n",
        "        get_mean_activation(batch[\"text\"]).cpu()\n",
        "    )\n",
        "    labels.append(batch[\"label\"])\n",
        "\n",
        "activations = torch.concat(activations, dim=0)\n",
        "labels = torch.concat(labels, dim=0)\n",
        "remove_mask = activations.isnan().any(dim=1)    # Remove nans...\n",
        "activations = activations[~remove_mask]\n",
        "labels = labels[~remove_mask]\n",
        "\n",
        "#############\n",
        "# Train CAV #\n",
        "#############\n",
        "acts_np = activations.numpy()\n",
        "labels_np = labels.numpy()\n",
        "svc = LinearSVC(fit_intercept=False).fit(acts_np, labels_np)\n",
        "cavs = svc.coef_\n",
        "cavs = torch.from_numpy(cavs).to(model.device)\n",
        "cavs /= cavs.norm(dim=1, keepdim=True)\n",
        "cav_labels = list(\n",
        "    map(    # 3: Remove the int-label\n",
        "        lambda x: x[1],\n",
        "        sorted(     # 2: Sort them in a list\n",
        "            list(\n",
        "                set(    # 1: Get unique int-label and str-label pairs\n",
        "                    map(\n",
        "                        lambda x: (x['label'],x['label_text']),\n",
        "                        ds['train']\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "assert len(cav_labels) == len(set(cav_labels))  # We've got a problem if there are any duplicates!\n",
        "\n",
        "\n",
        "# Purge model from memory\n",
        "import gc\n",
        "del model_base\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b8a8623",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "Let's run a small evaluation of the model steering in a fashion similar to [Wu et al., 2025](https://arxiv.org/abs/2501.17148).\n",
        "\n",
        "Try forming small groups and see if you can reach consensus on how well the LLM is able to do the following:\n",
        "- To follow the instruction you give it in the prompt\n",
        "- To talk about the chosen topic you're eliciting via the SAE feature/CAV, without you soliciting it in the prompt\n",
        "- To produce text in well-formed English (as you may have noticed, too much steering seems to break the model) \n",
        "\n",
        "In order to do so, write down a few different prompts you would like to investigate, and pick a few different labels for the SAE and the CAV (notice that they do not share all the same labels). For the strength parameter, tune it to be as optimal as possible (subject to the consensus of the group). Then rate the abovementioned points with an integer 0, 1, or 2 for different intervention strengths. You can aggregate them using the harmonic mean, i.e. \n",
        "$$\n",
        "\\textrm{score}_{\\rm aggregated}=(\\textrm{score}_{\\rm prompt}^{-1}+\\textrm{score}_{\\rm topic}^{-1} + \\textrm{score}_{\\rm well-formed}^{-1})^{-1}.\n",
        "$$\n",
        "Finally, you may take the average of the aggregated scores to compute your performance metric for the two different approaches.\n",
        "\n",
        "Is it difficult to reach consensus? And based on the aggregated scores, which of the two methods scores the highest in your experiment?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sae_labelling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
