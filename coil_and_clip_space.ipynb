{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "import open_clip\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = Path('data')\n",
    "\n",
    "# Set preferences\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 50 objects with 72 angles each\n"
     ]
    }
   ],
   "source": [
    "# Load all required data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load CLIP embeddings and metadata\n",
    "clip_file = DATA_PATH / 'CLIP_ViT-H-14_embeddings_structured.npz'\n",
    "with np.load(clip_file) as data:\n",
    "    clip_embeddings = data['embeddings'][:50]  # (50, 72, 1024)\n",
    "    object_nums = data['object_nums'][:50]     # sorted object IDs\n",
    "    angles = data['angles']               # sorted rotation angles\n",
    "\n",
    "# Load original images\n",
    "images_file = DATA_PATH / 'coil100_images_compressed.npz'\n",
    "with np.load(images_file) as data:\n",
    "    images = data['images'].astype(np.uint8)[:50]  # (50, 72, H, W, C)\n",
    "\n",
    "# Load PCA projections for both spaces\n",
    "input_pca_file = DATA_PATH / 'coil100_pca.npz'\n",
    "clip_pca_file = DATA_PATH / 'CLIP_ViT-H-14_pca.npz'\n",
    "input_pca = np.load(input_pca_file)['pca_projections'].astype(np.float32)[:50]  # (50, 72, 2)\n",
    "clip_pca = np.load(clip_pca_file)['pca_projections'].astype(np.float32)[:50]    # (50, 72, 2)\n",
    "\n",
    "# Load precomputed analysis matrices\n",
    "distances_file = DATA_PATH / 'clip_pca_mean_distances.npz'\n",
    "subspace_file = DATA_PATH / 'clip_pca_subspace_angles.npz'\n",
    "clip_mean_dist_matrix = np.load(distances_file)['mean_dist_matrix'][:50, :50]\n",
    "subspace_angle_matrix = np.load(subspace_file)['subspace_angle_matrix'][:50, :50]  # (50, 50, 2)\n",
    "\n",
    "# Create visualization helpers\n",
    "cmap = plt.get_cmap('twilight')\n",
    "angle_colors = cmap(np.linspace(0, 1, len(angles)))\n",
    "input_pca_dict = {int(object_nums[i]): input_pca[i] for i in range(len(object_nums))}\n",
    "clip_pca_dict = {int(object_nums[i]): clip_pca[i] for i in range(len(object_nums))}\n",
    "\n",
    "print(f\"Loaded {len(object_nums)} objects with {len(angles)} angles each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Interactive Visualizations Overview\n",
    "\n",
    "Welcome! In this notebook, you'll find a series of interactive visualizations designed to help you explore how objects are represented and transformed in both image and model spaces. We'll start simple and build up to more complex ideas, so you can get an intuitive feel for the data and what the model is learning.\n",
    "\n",
    "## 1. Input Space PCA Visualization: How Do Object Images Change as They Rotate?\n",
    "\n",
    "Let's begin by looking at the raw images themselves, before any neural network gets involved. This first interactive plot lets you see how the appearance of an object changes as it rotates, and how those changes look when we reduce the image data to just two dimensions using PCA (Principal Component Analysis).\n",
    "\n",
    "**How to use this plot:**\n",
    "- Use the dropdown menu to pick any object (from 1 to 50).\n",
    "- Use the slider to choose a rotation angle for that object.\n",
    "\n",
    "**What you'll see:**\n",
    "- On the left: the actual image of the selected object at your chosen angle.\n",
    "- On the right: a scatter plot showing all 72 images of that object (one for each angle), each as a point in 2D PCA space. The currently selected angle is highlighted.\n",
    "\n",
    "**What to notice:**\n",
    "- For most objects, the points will form a circle or loop. This means that as the object rotates, its pixel values change smoothly and regularlyâ€”a pattern that PCA captures nicely.\n",
    "- Try picking different objects and angles. Do all objects form nice circles? Are there exceptions?\n",
    "\n",
    "**Why this matters:** If rotation already creates a clear, simple structure in the raw pixel space, it gives us a baseline for what we might expect (or not expect) to see in deeper model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699daaae9a6947df84bf69ae512c0321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Object:', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f99aa95b07e420799b49d9a32d8e21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widget selectors for input space visualization\n",
    "object_selector = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[0]),\n",
    "    description='Object:',\n",
    "    continuous_update=False\n",
    ")\n",
    "angle_slider = widgets.IntSlider(\n",
    "    min=int(angles[0]),\n",
    "    max=int(angles[-1]),\n",
    "    step=5,\n",
    "    value=int(angles[0]),\n",
    "    description='Angle:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "def show_image_and_pca(obj_num, angle):\n",
    "    \"\"\"Display an object image and its PCA projection across all angles.\"\"\"\n",
    "    try:\n",
    "        i = np.where(object_nums == obj_num)[0][0]\n",
    "        j = np.where(angles == angle)[0][0]\n",
    "        img = images[i, j]\n",
    "        projs = input_pca_dict[obj_num]  # shape: (72, 2)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        # Show image\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].axis('off')\n",
    "        axs[0].set_title(f'Object {obj_num}, Angle {angle}Â°')\n",
    "\n",
    "        # Show PCA projection\n",
    "        axs[1].scatter(projs[:, 0], projs[:, 1], c=angle_colors, alpha=0.5, s=40)\n",
    "        axs[1].scatter(projs[j, 0], projs[j, 1], color=angle_colors[j], s=120,\n",
    "                       edgecolor='black', alpha=1, label='Selected angle')\n",
    "        axs[1].set_xlabel('PCA 1')\n",
    "        axs[1].set_ylabel('PCA 2')\n",
    "        axs[1].set_title('Input Space PCA Projections')\n",
    "        axs[1].axis('equal')\n",
    "        axs[1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except IndexError:\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.text(0.5, 0.5, 'Data not found', ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Interactive display\n",
    "ui = widgets.HBox([object_selector, angle_slider])\n",
    "out = widgets.interactive_output(show_image_and_pca, {'obj_num': object_selector, 'angle': angle_slider})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Visual and CLIP Representations: How Does CLIP \"See\" Rotations?\n",
    "\n",
    "In this section, you can explore how CLIP interprets object rotations compared to the original images. The interactive plot below lets you pick any object and rotation angle to see how its appearance and representation change.\n",
    "\n",
    "**How to use this plot:**  \n",
    "- Use the dropdown to select an object.  \n",
    "- Adjust the angle slider to rotate the object.  \n",
    "- Watch how the three panels update:\n",
    "     - **Left:** The actual image of the object at your chosen angle.\n",
    "     - **Middle:** The object's rotation pattern in pixel (input) space, visualized with PCA.\n",
    "     - **Right:** The same rotation pattern, but as CLIP \"sees\" itâ€”showing how CLIP transforms visual information into its own semantic space.\n",
    "\n",
    "**What to look for:**  \n",
    "- Notice how the CLIP space (right panel) often reshapes the rotation pattern. Sometimes it stays circular, but often it bends into arcs, horseshoes, or lines.  \n",
    "- Try different objectsâ€”smooth, textured, simple, or complexâ€”and see how their patterns change. This reveals how CLIP balances raw visual features with its learned, semantic understanding of objects.\n",
    "\n",
    "Take a moment to play with the controls and see which objects keep their shape in CLIP space, and which ones get transformed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24c3b259f2d40aa91455cc7585695a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Object:', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26c893e62074f2497ef361262b637a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widget selectors for comparison\n",
    "object_selector_comp = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[0]),\n",
    "    description='Object:',\n",
    "    continuous_update=False\n",
    ")\n",
    "angle_slider_comp = widgets.IntSlider(\n",
    "    min=int(angles[0]),\n",
    "    max=int(angles[-1]),\n",
    "    step=5,\n",
    "    value=int(angles[0]),\n",
    "    description='Angle:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "def show_combined_pca(obj_num, angle):\n",
    "    \"\"\"Compare object representation in input space vs CLIP space.\"\"\"\n",
    "    try:\n",
    "        i = np.where(object_nums == obj_num)[0][0]\n",
    "        j = np.where(angles == angle)[0][0]\n",
    "\n",
    "        img = images[i, j]\n",
    "        input_proj = input_pca_dict[obj_num]\n",
    "        clip_proj = clip_pca_dict[obj_num]\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "        # Original image\n",
    "        axs[0].imshow(img)\n",
    "        axs[0].axis('off')\n",
    "        axs[0].set_title(f'Object {obj_num}, Angle {angle}Â°')\n",
    "\n",
    "        # Input space PCA\n",
    "        axs[1].scatter(input_proj[:, 0], input_proj[:, 1], c=angle_colors, alpha=0.5, s=40)\n",
    "        axs[1].scatter(input_proj[j, 0], input_proj[j, 1], color=angle_colors[j], s=120,\n",
    "                       edgecolor='black', label='Selected angle')\n",
    "        axs[1].set_title('Input Space PCA')\n",
    "        axs[1].set_xlabel('PCA 1')\n",
    "        axs[1].set_ylabel('PCA 2')\n",
    "        axs[1].axis('equal')\n",
    "        axs[1].legend()\n",
    "\n",
    "        # CLIP space PCA\n",
    "        axs[2].scatter(clip_proj[:, 0], clip_proj[:, 1], c=angle_colors, alpha=0.5, s=40)\n",
    "        axs[2].scatter(clip_proj[j, 0], clip_proj[j, 1], color=angle_colors[j], s=120,\n",
    "                       edgecolor='black', label='Selected angle')\n",
    "        axs[2].set_title('CLIP Space PCA')\n",
    "        axs[2].set_xlabel('PCA 1')\n",
    "        axs[2].set_ylabel('PCA 2')\n",
    "        axs[2].axis('equal')\n",
    "        axs[2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except IndexError:\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.text(0.5, 0.5, 'Data not found', ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Interactive display\n",
    "ui = widgets.HBox([object_selector_comp, angle_slider_comp])\n",
    "out = widgets.interactive_output(show_combined_pca, {'obj_num': object_selector_comp, 'angle': angle_slider_comp})\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing How Objects Encode Rotation: Subspace Angles\n",
    "\n",
    "In this section, we explore how different objects represent rotation in CLIP space by comparing the orientation of their 2D rotational subspaces. This helps us understand whether objects that are close together in CLIP space also \"encode\" rotation in similar ways.\n",
    "\n",
    "**What does the interactive plot show?**  \n",
    "The interactive plot below lets you select two objects and a viewing angle. For each object, you'll see:\n",
    "- The image of the object at the chosen angle.\n",
    "- A visualization of its 2D rotation subspace in CLIP space.\n",
    "\n",
    "The plot also computes the *maximum principal angle* between the two objects' rotation subspaces. This angle tells us how well-aligned the two rotation planes are:  \n",
    "- **Smaller angles** mean the objects encode rotation in a more similar way.\n",
    "- **Larger angles** mean their rotation subspaces are more different.\n",
    "\n",
    "**How to use the plot:**  \n",
    "- Use the dropdown menus to pick two objects (Object X and Object Y) to compare.\n",
    "- Use the angle slider to choose a specific rotation angle for both objects.\n",
    "- The plot will update to show the selected images, their subspaces, and the alignment score.\n",
    "\n",
    "**Background:**  \n",
    "To compare the subspaces, we use principal angles computed via SVD. For two objects' subspace bases $Q_A$ and $Q_B$, the principal angles $\\theta_1, \\theta_2$ are given by:\n",
    "$$\n",
    "M = Q_A^\\top Q_B \\in \\mathbb{R}^{2\\times 2},\\quad M = U\\Sigma V^\\top,\\quad \\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2)\n",
    "$$\n",
    "where $\\sigma_i = \\cos\\theta_i$. The **maximum principal angle** $\\Theta_{\\max} = \\max(\\theta_1, \\theta_2)$ is used as a conservative measure of alignment.\n",
    "\n",
    "**Try it out:**  \n",
    "See if objects that are visually or semantically similar also have better-aligned rotation subspaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bb514da14440c6b3d5ba72f8ea0d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Object X:', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af314ef96414852a53e5b766b7b8c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widget selectors for subspace angle comparison\n",
    "object_selector_x = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[0]),\n",
    "    description='Object X:',\n",
    "    continuous_update=False\n",
    ")\n",
    "object_selector_y = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[1]),\n",
    "    description='Object Y:',\n",
    "    continuous_update=False\n",
    ")\n",
    "angle_slider_sub = widgets.IntSlider(\n",
    "    min=int(angles[0]),\n",
    "    max=int(angles[-1]),\n",
    "    step=5,\n",
    "    value=int(angles[0]),\n",
    "    description='Angle:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "def show_subspace_comparison(obj_num_x, obj_num_y, angle):\n",
    "    \"\"\"Compare two objects and their subspace angle in CLIP space.\"\"\"\n",
    "    try:\n",
    "        ix = np.where(object_nums == obj_num_x)[0][0]\n",
    "        iy = np.where(object_nums == obj_num_y)[0][0]\n",
    "        j = np.where(angles == angle)[0][0]\n",
    "        \n",
    "        img_x = images[ix, j]\n",
    "        img_y = images[iy, j]\n",
    "        clip_pca_x = clip_pca_dict[obj_num_x]\n",
    "        clip_pca_y = clip_pca_dict[obj_num_y]\n",
    "\n",
    "        # Get maximum subspace angle\n",
    "        max_angle_deg = np.max(subspace_angle_matrix[ix, iy])\n",
    "\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(18, 4))\n",
    "        \n",
    "        # Object X image\n",
    "        axs[0].imshow(img_x)\n",
    "        axs[0].axis('off')\n",
    "        axs[0].set_title(f'Object {obj_num_x}')\n",
    "        \n",
    "        # Object X PCA\n",
    "        axs[1].scatter(clip_pca_x[:, 0], clip_pca_x[:, 1], c=angle_colors, alpha=0.5, s=40)\n",
    "        axs[1].scatter(clip_pca_x[j, 0], clip_pca_x[j, 1], color=angle_colors[j], s=120,\n",
    "                       edgecolor='black', alpha=1, label=f'Angle {angle}Â°')\n",
    "        axs[1].set_xlabel('PCA 1')\n",
    "        axs[1].set_ylabel('PCA 2')\n",
    "        axs[1].set_title(f'Object {obj_num_x} CLIP PCA')\n",
    "        axs[1].axis('equal')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        # Object Y PCA\n",
    "        axs[2].scatter(clip_pca_y[:, 0], clip_pca_y[:, 1], c=angle_colors, alpha=0.5, s=40)\n",
    "        axs[2].scatter(clip_pca_y[j, 0], clip_pca_y[j, 1], color=angle_colors[j], s=120,\n",
    "                       edgecolor='black', alpha=1, label=f'Angle {angle}Â°')\n",
    "        axs[2].set_xlabel('PCA 1')\n",
    "        axs[2].set_ylabel('PCA 2')\n",
    "        axs[2].set_title(f'Object {obj_num_y} CLIP PCA')\n",
    "        axs[2].axis('equal')\n",
    "        axs[2].legend()\n",
    "        \n",
    "        # Object Y image\n",
    "        axs[3].imshow(img_y)\n",
    "        axs[3].axis('off')\n",
    "        axs[3].set_title(f'Object {obj_num_y}')\n",
    "        \n",
    "        plt.suptitle(f'Maximum Subspace Angle: {max_angle_deg:.2f}Â°', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except IndexError:\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.text(0.5, 0.5, 'Data not found', ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Interactive display\n",
    "ui = widgets.HBox([object_selector_x, angle_slider_sub, object_selector_y])\n",
    "out = widgets.interactive_output(\n",
    "    show_subspace_comparison, \n",
    "    {'obj_num_x': object_selector_x, 'obj_num_y': object_selector_y, 'angle': angle_slider_sub}\n",
    ")\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subspace Angle vs Distance: Exploring Relationships in CLIP Space\n",
    "\n",
    "Let's explore how similar or different objects are in CLIP's representation space, and whether that relates to how their \"rotation subspaces\" are oriented. In other words: do objects that are close together in CLIP space also rotate in similar ways?\n",
    "\n",
    "**What you'll see below:**  \n",
    "- Each object is represented by a set of CLIP embeddings, one for each rotation angle.\n",
    "- For each object, we compute the average embedding ($\\mu_i$) and the main plane of rotation in CLIP space (using PCA).\n",
    "- For every pair of objects, we calculate:\n",
    "    - The distance between their average embeddings ($d_{ij}$).\n",
    "    - The maximum angle between their rotation planes ($\\Theta_{ij}$).\n",
    "\n",
    "**How to use the interactive plot:**  \n",
    "- Use the dropdowns to select two objects.\n",
    "- The plot will show their distance in CLIP space and the angle between their rotation subspaces.\n",
    "- Try different pairs to see if objects that are close together also have more similarly oriented rotation subspaces.\n",
    "\n",
    "This helps us test the hypothesis: *Objects that are close in CLIP space tend to have more similarly oriented rotational subspaces.*\n",
    "\n",
    "Mathematically:\n",
    "- For object $i$ with embeddings $\\{f(x_i^\\theta)\\}$:\n",
    "  $$\n",
    "  \\mu_i = \\frac{1}{|\\Theta|}\\sum_{\\theta} f(x_i^\\theta),\\qquad U_i\\in\\mathbb{R}^{d\\times 2}\\ \\text{(PCA basis of its rotation plane)}.\n",
    "  $$\n",
    "- For a pair $(i,j)$:\n",
    "  $$\n",
    "  d_{ij}=\\lVert \\mu_i - \\mu_j\\rVert_2,\\qquad\n",
    "  \\Theta_{ij}=\\max\\big(\\arccos(\\sigma_1),\\arccos(\\sigma_2)\\big)\\ \\text{from } U_i^\\top U_j.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise statistics for scatter plot\n",
    "n = len(object_nums)\n",
    "pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n",
    "distances = []\n",
    "angles_max = []\n",
    "\n",
    "for i, j in pairs:\n",
    "    distances.append(clip_mean_dist_matrix[i, j])\n",
    "    angles_max.append(np.max(subspace_angle_matrix[i, j]))\n",
    "    \n",
    "distances = np.array(distances)\n",
    "angles_max = np.array(angles_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367afcdcfb8e4fbeb9d205649e82674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Object X:', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ccbdafed2e4859878d63eea67ac9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widget selectors for distance analysis\n",
    "object_selector_dist_x = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[0]),\n",
    "    description='Object X:',\n",
    "    continuous_update=False\n",
    ")\n",
    "object_selector_dist_y = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[1]),\n",
    "    description='Object Y:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "# Output widgets for images\n",
    "img_out_x = widgets.Output()\n",
    "img_out_y = widgets.Output()\n",
    "\n",
    "def update_images(obj_num_x, obj_num_y):\n",
    "    \"\"\"Update the displayed images for selected objects.\"\"\"\n",
    "    img_out_x.clear_output(wait=True)\n",
    "    img_out_y.clear_output(wait=True)\n",
    "    \n",
    "    with img_out_x:\n",
    "        try:\n",
    "            ix = np.where(object_nums == obj_num_x)[0][0]\n",
    "            img_x = images[ix, 0]\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(img_x)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Object {obj_num_x}')\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "    with img_out_y:\n",
    "        try:\n",
    "            iy = np.where(object_nums == obj_num_y)[0][0]\n",
    "            img_y = images[iy, 0]\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.imshow(img_y)\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Object {obj_num_y}')\n",
    "            plt.show()\n",
    "        except Exception:\n",
    "            plt.figure(figsize=(3, 3))\n",
    "            plt.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "def plot_distribution(obj_num_x, obj_num_y):\n",
    "    \"\"\"Plot distance vs angle distribution with selected pair highlighted.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Plot all pairs\n",
    "    ax.scatter(distances, angles_max, alpha=0.3, s=30, label='All pairs')\n",
    "    \n",
    "    # Highlight selected pair\n",
    "    try:\n",
    "        ix = np.where(object_nums == obj_num_x)[0][0]\n",
    "        iy = np.where(object_nums == obj_num_y)[0][0]\n",
    "        if ix < iy:\n",
    "            sel_idx = pairs.index((ix, iy))\n",
    "        else:\n",
    "            sel_idx = pairs.index((iy, ix))\n",
    "        ax.scatter([distances[sel_idx]], [angles_max[sel_idx]], \n",
    "                  color='red', s=150, label='Selected pair', \n",
    "                  edgecolor='black', linewidth=2, zorder=5)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    ax.set_xlabel('Mean Euclidean Distance in CLIP PCA Space', fontsize=12)\n",
    "    ax.set_ylabel('Maximum Subspace Angle (degrees)', fontsize=12)\n",
    "    ax.set_title('Relationship Between Distance and Subspace Angle', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set up observers\n",
    "object_selector_dist_x.observe(lambda c: update_images(object_selector_dist_x.value, object_selector_dist_y.value), names='value')\n",
    "object_selector_dist_y.observe(lambda c: update_images(object_selector_dist_x.value, object_selector_dist_y.value), names='value')\n",
    "\n",
    "# Initial image display\n",
    "update_images(object_selector_dist_x.value, object_selector_dist_y.value)\n",
    "\n",
    "# Layout\n",
    "vbox_x = widgets.VBox([object_selector_dist_x, img_out_x])\n",
    "vbox_y = widgets.VBox([object_selector_dist_y, img_out_y])\n",
    "top_row = widgets.HBox([vbox_x, vbox_y])\n",
    "\n",
    "# Interactive plot\n",
    "out = widgets.interactive_output(\n",
    "    plot_distribution, \n",
    "    {'obj_num_x': object_selector_dist_x, 'obj_num_y': object_selector_dist_y}\n",
    ")\n",
    "\n",
    "display(top_row, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring CLIP: Maximizing Text-Image Similarity\n",
    "Here, you can see how CLIP connects images and text by mapping both into a shared space and measuring how well they match. The goal is to experiment with different text descriptions for a given image and see which ones CLIP thinks are the best match.\n",
    "\n",
    "**How to use this interactive plot:**\n",
    "- Select an object from the dropdown menu to view its image.\n",
    "- In the text box, type a description you think matches the image.\n",
    "- Click \"Compute Similarity\" to see how closely your text matches the image according to CLIP.\n",
    "- Try different descriptions to see what features or words make the similarity score go up or down.\n",
    "\n",
    "This tool helps you understand what kinds of details or phrases CLIP pays attention to when matching images and text. It's a fun way to probe the model's \"thought process\" and see how it interprets both visual and textual information.\n",
    "\n",
    "**A bit of context:**  \n",
    "CLIP works by comparing L2-normalized image and text embeddings using a scaled dot product (often cosine similarity). During training, it learns to bring matching image-text pairs closer together and push mismatched pairs apart. Here, we're interested in the similarity score between your text and the selected imageâ€”the higher the score, the better the match!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize CLIP model\n",
    "print(\"Loading CLIP model...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-H-14\", pretrained=\"laion2b_s32b_b79k\"\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-H-14')\n",
    "model = model.float().eval().to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90130b79d0244afaa479a4d86be85a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Object:', options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e43cef20eca4764b37df25290aa7227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0f9e025b084f31b62f0b40365fdf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create widgets\n",
    "object_selector_sim = widgets.Dropdown(\n",
    "    options=[int(o) for o in object_nums],\n",
    "    value=int(object_nums[0]),\n",
    "    description='Object:',\n",
    ")\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Describe what you see...',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "compute_button = widgets.Button(\n",
    "    description='Compute Similarity',\n",
    "    button_style='primary',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "image_output = widgets.Output()\n",
    "results_output = widgets.Output()\n",
    "\n",
    "# History storage for each object\n",
    "similarity_history = {}\n",
    "\n",
    "def show_selected_image(obj_num):\n",
    "    \"\"\"Display the selected object image.\"\"\"\n",
    "    image_output.clear_output(wait=True)\n",
    "    with image_output:\n",
    "        ix = np.where(object_nums == obj_num)[0][0]\n",
    "        img = images[ix, 0]  # Use first angle\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Object {obj_num}')\n",
    "        plt.show()\n",
    "\n",
    "def compute_similarity():\n",
    "    \"\"\"Compute CLIP similarity between image and text.\"\"\"\n",
    "    obj_num = object_selector_sim.value\n",
    "    description = text_input.value.strip()\n",
    "    \n",
    "    if not description:\n",
    "        return\n",
    "    \n",
    "    # Get image\n",
    "    ix = np.where(object_nums == obj_num)[0][0]\n",
    "    img = images[ix, 0]\n",
    "    pil_img = Image.fromarray(img)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        # Image embedding\n",
    "        image_input = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image_input)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Text embedding\n",
    "        text_input_tensor = tokenizer([description]).to(device)\n",
    "        text_features = model.encode_text(text_input_tensor)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "    \n",
    "    # Store in history\n",
    "    if obj_num not in similarity_history:\n",
    "        similarity_history[obj_num] = []\n",
    "    similarity_history[obj_num].append((description, similarity))\n",
    "    \n",
    "    # Clear input\n",
    "    text_input.value = ''\n",
    "    \n",
    "    # Update display\n",
    "    update_results_display()\n",
    "\n",
    "def update_results_display():\n",
    "    \"\"\"Update the results display with history and plot.\"\"\"\n",
    "    results_output.clear_output(wait=True)\n",
    "    with results_output:\n",
    "        obj_num = object_selector_sim.value\n",
    "        if obj_num in similarity_history and similarity_history[obj_num]:\n",
    "            history = similarity_history[obj_num]\n",
    "            \n",
    "            # Show last 5 attempts\n",
    "            print(\"Recent attempts:\")\n",
    "            print(\"-\" * 50)\n",
    "            for i, (desc, sim) in enumerate(history[-5:], 1):\n",
    "                print(f\"{i}. \\\"{desc}\\\"\")\n",
    "                print(f\"   Similarity: {sim:.4f}\")\n",
    "                print()\n",
    "            \n",
    "            # Find best attempt\n",
    "            best_desc, best_sim = max(history, key=lambda x: x[1])\n",
    "            print(f\"Best attempt: \\\"{best_desc}\\\"\")\n",
    "            print(f\"Best similarity: {best_sim:.4f}\")\n",
    "            \n",
    "            # Plot progress\n",
    "            if len(history) > 1:\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                attempts = range(1, len(history) + 1)\n",
    "                similarities = [sim for _, sim in history]\n",
    "                \n",
    "                plt.plot(attempts, similarities, 'b-o', markersize=8, linewidth=2)\n",
    "                plt.xlabel('Attempt Number', fontsize=12)\n",
    "                plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "                plt.title(f'Similarity Progress for Object {obj_num}', fontsize=14)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.ylim(-0.1, 1.1)\n",
    "                \n",
    "                # Highlight best attempt\n",
    "                best_idx = np.argmax(similarities)\n",
    "                plt.plot(best_idx + 1, similarities[best_idx], 'r*', markersize=20)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "def on_object_change(change):\n",
    "    \"\"\"Handle object selection change.\"\"\"\n",
    "    show_selected_image(object_selector_sim.value)\n",
    "    update_results_display()\n",
    "\n",
    "def on_button_click(b):\n",
    "    \"\"\"Handle compute button click.\"\"\"\n",
    "    compute_similarity()\n",
    "\n",
    "def on_text_submit(change):\n",
    "    \"\"\"Handle text input submission (Enter key).\"\"\"\n",
    "    if not change['new'] and not change['old']:\n",
    "        # Ignore initial trigger\n",
    "        return\n",
    "    if change['name'] == 'value' and change['type'] == 'change':\n",
    "        compute_similarity()\n",
    "\n",
    "# Connect event handlers\n",
    "object_selector_sim.observe(on_object_change, names='value')\n",
    "compute_button.on_click(on_button_click)\n",
    "# Set .continuous_update to False and observe value changes for text_input\n",
    "text_input.continuous_update = False\n",
    "text_input.observe(on_text_submit, names='value')\n",
    "\n",
    "# Initial display\n",
    "show_selected_image(object_selector_sim.value)\n",
    "\n",
    "# Create layout\n",
    "input_row = widgets.HBox([object_selector_sim, text_input, compute_button])\n",
    "display(input_row)\n",
    "display(image_output)\n",
    "display(results_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
